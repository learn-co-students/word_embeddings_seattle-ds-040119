{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NLP - Word Embeddings\n",
    "\n",
    "Think back to NLP as we've understood it so far.\n",
    "\n",
    "If we've had some luck with NLP modeling, likely with a NaiveBayes algorithm, we were able to illustrate some correlations between words and some other feature of interest.\n",
    "\n",
    "But to whatever extent that our models were able to make connections and pick up on correlations, they did this *without any understanding of the **meaning** of the words in question*.\n",
    "\n",
    "Let's think for a minute about words and objective meanings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make sense of meaning for computational purposes by thinking about meaning in terms of similarity, i.e. thinking about meaning *holistically*.\n",
    "\n",
    "Q. Is there any precedent for this way of thinking about meaning? <br/>\n",
    "A. [Yes](https://plato.stanford.edu/entries/meaning-holism/#ArgForMeaHol)\n",
    "\n",
    "So what will this look like for us?\n",
    "\n",
    "*Remember cosine similarity?*\n",
    "\n",
    "$\\rightarrow$We'll have much the same idea here: Associate each word with values along particular dimensions in a multi-dimensional space. If we had a dimension for *softness*, for example, then pillows and marshmallows would score higher on it than rocks and bricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "\n",
    "import json\n",
    "\n",
    "with open('JEOPARDY_QUESTIONS1.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216930"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'HISTORY',\n",
       " 'air_date': '2004-12-31',\n",
       " 'question': \"'For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory'\",\n",
       " 'value': '$200',\n",
       " 'answer': 'Copernicus',\n",
       " 'round': 'Jeopardy!',\n",
       " 'show_number': '4680'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first element in our list\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words do we have?\n",
    "\n",
    "len(data[0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'For\",\n",
       " 'the',\n",
       " 'last',\n",
       " '8',\n",
       " 'years',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life,',\n",
       " 'Galileo',\n",
       " 'was',\n",
       " 'under',\n",
       " 'house',\n",
       " 'arrest',\n",
       " 'for',\n",
       " 'espousing',\n",
       " 'this',\n",
       " \"man's\",\n",
       " \"theory'\"]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try that again!\n",
    "\n",
    "\n",
    "\n",
    "data[0]['question'].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]['question'].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169994"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = 0\n",
    "for clue in data:\n",
    "    length += len(clue['question'].split(' '))\n",
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec requires that our text have the form of a list\n",
    "# of 'sentences', where each sentence is itself a list of\n",
    "# words. How can we put our _Jeopardy!_ clues in that shape?\n",
    "\n",
    "import string\n",
    "text = []\n",
    "\n",
    "for clue in data:\n",
    "    sentence = clue['question'].translate(str.maketrans('', '',\n",
    "                                                        string.punctuation)).split(' ')\n",
    "    \n",
    "    new_sent = []\n",
    "    for word in sentence:\n",
    "        new_sent.append(word.lower())\n",
    "    \n",
    "    text.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for',\n",
       " 'the',\n",
       " 'last',\n",
       " '8',\n",
       " 'years',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life',\n",
       " 'galileo',\n",
       " 'was',\n",
       " 'under',\n",
       " 'house',\n",
       " 'arrest',\n",
       " 'for',\n",
       " 'espousing',\n",
       " 'this',\n",
       " 'mans',\n",
       " 'theory']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the new structure of our first clue\n",
    "\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "King + Woman - Man = Queen\n",
    "\n",
    "Brother + Woman - Man = Sister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model is simply a matter of\n",
    "# instantiating a Word2Vec object.\n",
    "\n",
    "model = gensim.models.Word2Vec(text, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11335257, 15849970)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To train, call 'train()'!\n",
    "\n",
    "model.train(text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169994"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1a537b36a0>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The '.wv' attribute stores the word vectors\n",
    "\n",
    "model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13272567, -0.9963929 , -1.009114  ,  0.7415139 , -0.09692774,\n",
       "        0.6901204 , -2.525515  ,  0.47451624,  0.49218813, -1.3115813 ,\n",
       "       -0.06093341, -0.75593024, -0.6008816 ,  0.7677939 , -0.9035876 ,\n",
       "        0.46432412, -1.2208815 ,  0.67908645,  0.05308787, -0.05163215,\n",
       "       -0.66670424,  0.06529696, -0.33506417, -0.6209905 , -0.3127922 ,\n",
       "       -0.08252732, -2.3465168 ,  0.46304706,  1.259904  ,  0.5548379 ,\n",
       "        1.3921033 ,  0.25856656, -0.69543123,  0.77259916, -1.1132499 ,\n",
       "        0.40458286,  0.14111084,  0.38960007, -0.11525829,  0.2195491 ,\n",
       "       -0.22003417, -0.78036827,  0.61816347, -0.89490986, -0.25579047,\n",
       "        0.5786734 , -0.11730018, -0.09900374,  1.1034496 ,  2.647091  ,\n",
       "       -1.0017473 ,  0.861824  ,  0.7086641 ,  0.4205222 ,  0.81788063,\n",
       "        1.2102062 ,  0.61050105,  0.15725121, -0.33728507, -0.76713985,\n",
       "        0.99270153, -0.88737583, -0.44358185,  0.56268024,  1.5707555 ,\n",
       "        0.01699882,  0.51790047, -0.24234052, -0.2508243 ,  0.15403834,\n",
       "        0.43191776,  0.47798514,  0.71658474,  0.45114756, -0.15510622,\n",
       "        0.38591453,  1.4054581 , -1.8331317 , -1.3678948 ,  0.12407978,\n",
       "       -0.578128  , -1.5793663 ,  1.0095412 , -0.37613845, -0.8308174 ,\n",
       "        1.2204723 ,  0.7574295 ,  1.260722  ,  0.6162676 ,  0.03453236,\n",
       "        0.33822837,  0.3642228 ,  1.3380556 , -1.3373284 , -0.43662623,\n",
       "       -3.0500553 , -0.39067724,  0.43980658,  1.4465215 , -0.70422566],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['child']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.wv methods\n",
    "#### 'most_similar()' and 'similarity()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jewelry', 0.7482991218566895),\n",
       " ('plastic', 0.7242025136947632),\n",
       " ('fabric', 0.7209893465042114),\n",
       " ('clothing', 0.7190870046615601),\n",
       " ('decorative', 0.7137700915336609),\n",
       " ('folding', 0.6910587549209595),\n",
       " ('equipment', 0.6794615387916565),\n",
       " ('wood', 0.6783871054649353),\n",
       " ('baking', 0.6776121258735657),\n",
       " ('rubber', 0.6774500608444214)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('furniture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74829924"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('furniture', 'jewelry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('insect', 0.8420601487159729),\n",
       " ('bird', 0.8405530452728271),\n",
       " ('rodent', 0.8203315734863281),\n",
       " ('creature', 0.7947182655334473),\n",
       " ('lizard', 0.7780484557151794),\n",
       " ('feline', 0.761682391166687),\n",
       " ('fish', 0.7601139545440674),\n",
       " ('relative', 0.7579514980316162),\n",
       " ('reptile', 0.7565328478813171),\n",
       " ('marsupial', 0.7508257031440735)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['cat', 'animal', 'pet', 'mammal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bird', 0.5044184923171997),\n",
       " ('animals', 0.5022597312927246),\n",
       " ('birds', 0.4908377528190613),\n",
       " ('species', 0.48265960812568665),\n",
       " ('hunting', 0.48032307624816895),\n",
       " ('breed', 0.4796659052371979),\n",
       " ('mammal', 0.4792921543121338),\n",
       " ('dogs', 0.44013819098472595),\n",
       " ('dog', 0.4381997883319855),\n",
       " ('insect', 0.43811923265457153)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['cat', 'animal'], negative='pet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('empress', 0.49203425645828247),\n",
       " ('throne', 0.4811129570007324),\n",
       " ('queen', 0.480634868144989)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['king', 'woman'], negative='man', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pageant', 0.5714678764343262),\n",
       " ('fargo', 0.5668860673904419),\n",
       " ('minneapolis', 0.5316091775894165),\n",
       " ('90210', 0.5216965079307556),\n",
       " ('summer', 0.5198173522949219),\n",
       " ('bronx', 0.5194722414016724),\n",
       " ('edina', 0.5112684965133667),\n",
       " ('tonight', 0.5014094114303589),\n",
       " ('yesterday', 0.4972144365310669),\n",
       " ('bway', 0.4931708872318268)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='usa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hawaii', 0.7062503099441528),\n",
       " ('switzerland', 0.7037385702133179),\n",
       " ('territory', 0.7032467126846313),\n",
       " ('alaska', 0.6920301914215088),\n",
       " ('peru', 0.6817297339439392),\n",
       " ('colombia', 0.6571628451347351),\n",
       " ('pakistan', 0.6542963981628418),\n",
       " ('morocco', 0.644716739654541),\n",
       " ('japan', 0.6415838003158569),\n",
       " ('statehood', 0.6406919360160828)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('canada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hemingway', 0.6982564330101013),\n",
       " ('tennyson', 0.6815193295478821),\n",
       " ('hamlet', 0.6687659621238708),\n",
       " ('dickens', 0.6686815023422241),\n",
       " ('poe', 0.6508814692497253),\n",
       " ('shakespeares', 0.6488662958145142),\n",
       " ('macbeth', 0.6414898633956909),\n",
       " ('shelley', 0.6403170824050903),\n",
       " ('tolstoy', 0.6312527060508728),\n",
       " ('eliot', 0.6235682964324951)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kinnear', 0.851963460445404),\n",
       " ('hatcher', 0.8268362283706665),\n",
       " ('reese', 0.8254306316375732),\n",
       " ('2004br', 0.8243862986564636),\n",
       " ('det', 0.8158860802650452),\n",
       " ('1989br', 0.8155031204223633),\n",
       " ('kiefer', 0.8080366253852844),\n",
       " ('randy', 0.8043558597564697),\n",
       " ('jake', 0.8002606630325317),\n",
       " ('bobby', 0.7988304495811462)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('greg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quincy', 0.7526087760925293),\n",
       " ('madison', 0.7080703973770142),\n",
       " ('sen', 0.6974426507949829),\n",
       " ('dewey', 0.6706094741821289),\n",
       " ('hw', 0.6679573059082031),\n",
       " ('booker', 0.6666173934936523),\n",
       " ('josiah', 0.6663467884063721),\n",
       " ('marshall', 0.6623543500900269),\n",
       " ('rep', 0.6621801257133484),\n",
       " ('hoover', 0.6592093706130981)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('jefferson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lincoln', 0.6199996471405029),\n",
       " ('memorial', 0.6156197786331177),\n",
       " ('virginia', 0.5984926819801331),\n",
       " ('illinois', 0.5932536125183105),\n",
       " ('arlington', 0.5758087635040283),\n",
       " ('nebraska', 0.5739896297454834),\n",
       " ('missouri', 0.5725917816162109),\n",
       " ('texas', 0.5661277770996094),\n",
       " ('kansas', 0.565746545791626),\n",
       " ('montana', 0.5633023381233215)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('washington')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('february', 0.39004337787628174),\n",
       " ('remains', 0.38808560371398926),\n",
       " ('exile', 0.3841812312602997),\n",
       " ('dictator', 0.38287919759750366),\n",
       " ('1929', 0.379180908203125),\n",
       " ('1918', 0.37386244535446167),\n",
       " ('voyage', 0.37059006094932556),\n",
       " ('decade', 0.3700261414051056),\n",
       " ('russia', 0.3685533106327057),\n",
       " ('completed', 0.3664979338645935)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['president', 'germany'], negative='usa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exile', 0.3980652391910553),\n",
       " ('remains', 0.38099491596221924),\n",
       " ('voyage', 0.37491631507873535),\n",
       " ('shah', 0.36604753136634827),\n",
       " ('dictator', 0.35999220609664917),\n",
       " ('philippines', 0.3587689697742462),\n",
       " ('february', 0.3575618863105774),\n",
       " ('date', 0.3540005087852478),\n",
       " ('conquest', 0.35296034812927246),\n",
       " ('1793', 0.3519843816757202)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['president', 'france'], negative='usa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'doesnt_match()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['breakfast', 'lunch', 'frog', 'food'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lunch'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['lunch', 'this'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bush'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['tree', 'flower', 'bush', 'plant', 'toothbrush'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toothbrush'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['tree', 'flower', 'plant', 'toothbrush'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'closer_than()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prince', 'emperor', 'ruler']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are closer to 'king' than 'queen' is?\n",
    "\n",
    "model.wv.closer_than('king', 'queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'distance()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this it will make more sense to\n",
    "# normalize our vectors.\n",
    "\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.distance('king', 'king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4097934365272522"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.distance('joy', 'happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'evaluate_word_analogies()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [this text file](https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatives = model.wv.evaluate_word_analogies('https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt')[1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relatives['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relatives['incorrect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOY', 'GIRL', 'BROTHER', 'SISTER'),\n",
       " ('BOY', 'GIRL', 'BROTHERS', 'SISTERS'),\n",
       " ('BOY', 'GIRL', 'DAD', 'MOM'),\n",
       " ('BOY', 'GIRL', 'FATHER', 'MOTHER'),\n",
       " ('BOY', 'GIRL', 'HE', 'SHE')]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatives['correct'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOY', 'GIRL', 'GRANDFATHER', 'GRANDMOTHER'),\n",
       " ('BOY', 'GIRL', 'GRANDPA', 'GRANDMA'),\n",
       " ('BOY', 'GIRL', 'GRANDSON', 'GRANDDAUGHTER'),\n",
       " ('BOY', 'GIRL', 'GROOM', 'BRIDE'),\n",
       " ('BOY', 'GIRL', 'HUSBAND', 'WIFE')]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatives['incorrect'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
